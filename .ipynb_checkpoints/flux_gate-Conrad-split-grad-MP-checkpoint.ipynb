{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "from osgeo import gdal\n",
    "import salem\n",
    "from salem import *\n",
    "\n",
    "import fiona, rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.plot import show\n",
    "import rasterio.plot as rplt\n",
    "from rasterio.features import rasterize\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "import statsmodels.api as stm\n",
    "\n",
    "years = [2016, 2017, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(1,3, sharex=True, sharey=True, figsize=(7.08,3.34)) #obs. and flux gate SMB boxplot/errorbar plot\n",
    "fig2, ax2 = plt.subplots(1,3, sharex=True, sharey=True, figsize=(7.08,3.34)) ##mass conservation plot\n",
    "count = 0\n",
    "\n",
    "for yr in range(len(years)):\n",
    "    # season\n",
    "    balance = 'Ba'  # Bw or Ba (winter or annual)\n",
    "    # rho_snow = 445. # 470 kg m-3 geodetic or 457 glaciological, or...take given years obs. value\n",
    "    year = years[yr] # any of: 2016, 2017, 2018\n",
    "    I = np.abs(year - 2016) # for selecting raster inputs\n",
    "\n",
    "    vcorr=False ## is velocity corrected by off ice bias and stake bias?\n",
    "\n",
    "    gl = 1 # number to select glacier\n",
    "    Glacier = ['Kokanee', 'Conrad', 'Illecillewaet']\n",
    "    glacier = ['kokanee', 'conrad', 'illecillewaet']\n",
    "    d = 20. #20.  #distance (m) between flux gate points\n",
    "    Dint = 0.8 #depth integrated velocity ratio, 1.0= all sliding\n",
    "    sVZ = 4.91  #velocity against stake measurements\n",
    "    sHopt = 0.1 #10.3 ## ME:10.3 m,  MAE:0.223 (percent ratio)\n",
    "    cb = 7 ##center bin use this to assure flux in is from bins cb+1 and cb+2\n",
    "\n",
    "    fl_path = '/home/pelto/Desktop/ice_flux/' + Glacier[gl] + '/'\n",
    "    gpr_path = '/home/pelto/GIS/GPR/ComparisonData/'\n",
    "    path = '/home/pelto/Desktop/lidar_cbt_analysis/' + glacier[gl] + '/'\n",
    "\n",
    "    VDIR = 'average'  # 'average' or 'individual'\n",
    "\n",
    "    if VDIR == 'individual':  \n",
    "\n",
    "        ## DEMs\n",
    "        vf_list = ['img1_20160417__img1_20170521_vmap_10m_35px_spm2/', 'img1_20160417__img1_20170521_vmap_10m_35px_spm2/',\n",
    "               'img1_20170904__img2_20180822_vmap_10m_35px_spm2/'] #'img1_20170521__img2_20180426_vmap_10m_35px_spm2/']##backups  img1_20170917__img1_20180426_vmap_5m_35px_spm2/\n",
    "\n",
    "        ##planet labs\n",
    "#         vf_list = ['xx', 'xx',\n",
    "#                'img1_20170904__img2_20180822_vmap_10m_35px_spm2/']\n",
    "\n",
    "        vdir = '/home/pelto/Desktop/velocity_mapping/' +Glacier[gl] + '_DEMs/bedem5_spm2/' + vf_list[I]\n",
    "#         vdir = '/home/pelto/Desktop/velocity_mapping/conrad_planet/' + vf_list[I]\n",
    "        VX = vdir+ vf_list[I][:-1] + '-F_vx.tif' \n",
    "        VY = vdir+ vf_list[I][:-1] + '-F_vy.tif' \n",
    "\n",
    "    # # if VDIR == 'individual':  \n",
    "    #     vf_list = ['img1_20160417__img1_20170521_vmap_5.0m_35px_spm2/', 'img1_20160417__img1_20170521_vmap_5.0m_35px_spm2/',\n",
    "    #            'img1_20170521__img2_20180426_vmap_5.0m_35px_spm2/']##backups  img1_20170917__img1_20180426_vmap_5m_35px_spm2/\n",
    "    #     vdir = '/home/pelto/Desktop/velocity_mapping/' +Glacier[gl] + '_DEMs/spm2/' + vf_list[I]\n",
    "    #     VX = vdir+ vf_list[I][:-1] + '-F_vx.tif' \n",
    "    #     VY = vdir+ vf_list[I][:-1] + '-F_vy.tif' \n",
    "    else:\n",
    "    #     vf_list = ['illec_all_spm2_5m', 'illec_all_spm2_5m', 'illec_all_spm2_5m']\n",
    "        vdir = '/home/pelto/Desktop/velocity_mapping/Conrad_DEMs/spm2/3m/'#bedem5_spm2/'  \n",
    "        VX = vdir+ 'conrad_all_vx_3mdems+planet_5m.tif' #'conrad_vx_spring_10m.tif' \n",
    "        VY = vdir+ 'conrad_all_vy_3mdems+planet_5m.tif' #'conrad_vy_spring_10m.tif'\n",
    "        VM = vdir + 'conrad_all_vm_3mdems+planet_5m.tif'\n",
    "    ## velocity_mapping/conrad_planet/img1_20170904__img2_20180822_vmap_10m_35px_spm2\n",
    "\n",
    "    topo = '/home/pelto/Desktop/lidar_cbt_analysis/conrad/20160912_conrad_dem1_clip_slave.tif'\n",
    "    farinotti = gpr_path + 'RGI60-02.02171_thickness.tif'\n",
    "\n",
    "    dhW_list = ['conrad_2016_winter_dh_dt14s.tif', 'conrad_2017_winter_dh_dt16s.tif', 'conrad_2018_winter_dh_dt.tif']\n",
    "    dhA_list = ['conrad_2015_2016_dh_dt_filled_1416+50cm.tif', 'conrad_2016_2017_dh_dt_filled.tif','conrad_2018_2017_dh_dt_60.tif']\n",
    "    if balance == 'Bw':\n",
    "        dh_r = path+ dhW_list[I]   #winter height change TIFF\n",
    "    else:\n",
    "        dh_r = path+  dhA_list[I] #Annual height change TIFF\n",
    "\n",
    "    pts_file = fl_path + 'conrad_points_17gates_C_20m_wgs84_b.shp' #'conrad_points_17gates_20m_wgs84.shp'  ##must be WGS84 currently\n",
    "    gates = fl_path+'conrad_flux_gates_17_C.shp'\n",
    "    shpf = path + Glacier[gl] + '/conrad_all_glaciers_2014.shp'  #GLIMS_BC/glims_all/all_glaciers_2016.shp' \n",
    "    bins=fl_path+glacier[gl]+'_bins_2017_C'+'.shp' #+str(year)\n",
    "\n",
    "    obs = pd.read_csv(fl_path+ 'Conrad_bdot.csv') \n",
    "    rho = pd.read_csv(fl_path + 'conrad_rho.csv')  ## rho\n",
    "\n",
    "    # open GeoTIFF as array\n",
    "\n",
    "    vx = salem.open_xr_dataset(VX)  #gdal.Open(vx).ReadAsArray()\n",
    "    vy = salem.open_xr_dataset(VY)#gdal.Open(vy).ReadAsArray()\n",
    "    msk = salem.open_xr_dataset('/home/pelto/Desktop/lidar_cbt_analysis/conrad/conrad_total_msk.tif') \n",
    "    # msk = salem.open_xr_dataset('/home/pelto/Desktop/velocity_mapping/Conrad_DEMs/bedem5_spm2/conrad_2016_ice_msk_5m_glacieronly.tif') \n",
    "    msk_conrad = salem.open_xr_dataset('/home/pelto/Desktop/lidar_cbt_analysis/conrad/conrad_2014_extent_5m.tif')\n",
    "\n",
    "    gpr = salem.open_xr_dataset(fl_path + 'gpr_outline_25_100m_5m.tif') #'gpr_outlines_all_25_25m_re5m.tif'\n",
    "    farin = salem.open_xr_dataset(farinotti)\n",
    "\n",
    "    H_opt = salem.open_xr_dataset(fl_path+'opt_thick_101sw_MAE_5m.tif') \n",
    "    #     pts = salem.read_shapefile(fl_path+pts)\n",
    "    gates = salem.read_shapefile(gates)\n",
    "\n",
    "    vx = vx.to_array(name='vx')\n",
    "    # np.shape(vx[0]) remove index 1 i.e.: 1,2000,3000\n",
    "    vy = vy.to_array(name='vy')\n",
    "\n",
    "    gpr_reproj = vx.salem.transform(gpr)\n",
    "    gpr = gpr_reproj.to_array(name='gpr')\n",
    "\n",
    "    msk_reproj = vx.salem.transform(msk)  #note succeeding trying to use gdalwarp to go from 2955 --> 32611\n",
    "    msk = msk_reproj.to_array(name='msk')\n",
    "    msk_conrad_reproj = vx.salem.transform(msk_conrad)\n",
    "    msk_conrad = msk_conrad_reproj.to_array(name='msk_conrad')\n",
    "\n",
    "    # farin = farin.to_array(name='vx')\n",
    "    H_opt_reproj = vx.salem.transform(H_opt)\n",
    "    H_opt = H_opt_reproj.to_array(name='H_opt')\n",
    "\n",
    "    farin_reproj= vx.salem.transform(farin)\n",
    "    farin = farin_reproj.to_array(name='farin')\n",
    "    srtm_corr = fl_path + 'conrad_SRTM_diff_30m.tif'\n",
    "    srtm_corr = salem.open_xr_dataset(srtm_corr)\n",
    "    srtm_corr = vx.salem.transform(srtm_corr)\n",
    "    srtm_corr = srtm_corr.to_array(name='srtm_corr')\n",
    "    srtm_corr.data[srtm_corr.data>10.0] = 0.0  ##remove positive anomalous values\n",
    "    srtm_corr.data[srtm_corr.data<-50.0] = 0.0 ##remove negative anomalous values\n",
    "    farin_corr = farin + srtm_corr\n",
    "    farin_corr= farin_corr.rename('farin_corr')\n",
    "\n",
    "    gpr.data[gpr.data<0] = np.nan\n",
    "    gpr.data[gpr.data<1.5] = 0.0   #not having any effect\n",
    "    VZ = np.sqrt(vx.data**2 + vy.data**2 )\n",
    "    vx.data[msk.data==0] = np.nan\n",
    "    vy.data[msk.data==0] = np.nan\n",
    "\n",
    "    VZ_off_ice = VZ.copy()\n",
    "    VZ_off_ice[msk.data>0.0] = np.nan\n",
    "    VZ[msk_conrad.data<0.0] = np.nan  ##################### I'm HERE ################\n",
    "    # VZ_off_ice[VZ_off_ice==0.0] = np.nan\n",
    "\n",
    "    dem = salem.open_xr_dataset(topo)\n",
    "    dem_reproj = vx.salem.transform(dem)\n",
    "    dem = dem_reproj.to_array(name='dem')\n",
    "    dem.data[dem.data<1] = np.nan\n",
    "\n",
    "    #     # map extent\n",
    "    # grid = mercator_grid(center_ll=(-117.43, 51.238), extent=(4500, 4500)) ##zoomed out view\n",
    "    # 487892.000 5509738.000 491232.000 5512358.000\n",
    "    grid = vx.salem.grid  ##full view\n",
    "    sm = Map(grid, countries=False)\n",
    "    sm.set_lonlat_contours(interval=0)\n",
    "    sm.set_scale_bar()\n",
    "    sm.set_data(VZ_off_ice) #, label='m')\n",
    "    # sm.set_vmax(val=50.)\n",
    "    #      Change the lon-lat countour setting\n",
    "    sm.set_lonlat_contours(add_ytick_labels=True, interval=0.05, linewidths=0.75, linestyles='--', colors='0.25')\n",
    "    off_ice_V = np.nanmean(VZ_off_ice)\n",
    "    print(off_ice_V)\n",
    "    print(np.nanstd(VZ_off_ice))\n",
    "    \n",
    "    gdf = salem.read_shapefile(shpf)\n",
    "    sm.set_shapefile(gdf, linewidth=1)\n",
    "    sm.set_shapefile(gates, linewidth=1.5, color='r')\n",
    "#     sm.visualize()\n",
    "#     plt.savefig(fl_path+ 'products/'+ glacier[gl] +'_thickness_gates.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "    VZ.mean()\n",
    "\n",
    "    ## Functions for calculating zonal statistics over each flux gate bin\n",
    "\n",
    "    # https://community.esri.com/groups/python-snippets/blog/2019/05/07/calculating-zonal-statistics-with-python-rasterstats\n",
    "    # For loading shapefiles into geopandas dataframe\n",
    "\n",
    "    def enum_items(source):\n",
    "        print(\"\\n\")\n",
    "        for ele in enumerate(source): \n",
    "            print(ele)\n",
    "\n",
    "    def list_columns(df):\n",
    "        field_list = list(df)\n",
    "        enum_items(field_list)\n",
    "        return field_list\n",
    "\n",
    "    def loadshp_as_gpd(shp):\n",
    "        data_shp = gpd.read_file(shp)\n",
    "        return data_shp\n",
    "\n",
    "    # For loading feature classes into geopandas dataframe\n",
    "    def loadfc_as_gpd(fgdb):\n",
    "        layers = fiona.listlayers(fgdb)\n",
    "        enum_items(layers)\n",
    "        index = int(input(\"Which index to load? \"))\n",
    "        fcgpd = gpd.read_file(fgdb,layer=layers[index])\n",
    "        return fcgpd\n",
    "\n",
    "    # For re-projecting input vector layer to raster projection\n",
    "    def reproject(fcgpd, raster):\n",
    "        proj = raster.crs.to_proj4()\n",
    "        print(\"Original vector layer projection: \", fcgpd.crs)\n",
    "        reproj = fcgpd.to_crs(proj)\n",
    "        print(\"New vector layer projection (PROJ4): \", reproj.crs)\n",
    "#         fig, ax = plt.subplots(figsize=(15, 15))\n",
    "#         rplt.show(raster, ax=ax)\n",
    "#         reproj.plot(ax=ax, facecolor='none', edgecolor='red')\n",
    "#         fig.show()\n",
    "        return reproj\n",
    "\n",
    "    def dissolve_gpd(df):\n",
    "        field_list = list_columns(df)\n",
    "        index = 1 #int(input(\"Dissolve by which field (index)? \"))\n",
    "        dgpd = df.dissolve(by=field_list[index])\n",
    "        return dgpd\n",
    "\n",
    "    # For selecting which raster statistics to calculate\n",
    "    def stats_select():\n",
    "        stats_list = stats_list = ['min', 'max', 'mean', 'count', \n",
    "                  'sum', 'std', 'median', 'majority', \n",
    "                  'minority', 'unique', 'range']\n",
    "        enum_items(stats_list)\n",
    "    #     indices = input(\"Enter raster statistics selections separated by space: \")\n",
    "        indices='2 3 5 6'\n",
    "        stats  = list(indices.split())\n",
    "        out_stats = list()\n",
    "        for i in stats:\n",
    "            out_stats.append(stats_list[int(i)])\n",
    "        return out_stats\n",
    "\n",
    "    def get_zonal_stats(vector, raster, stats):\n",
    "        # Run zonal statistics, store result in geopandas dataframe\n",
    "        result = zonal_stats(vector, raster, stats=stats, geojson_out=True)\n",
    "        geostats = gpd.GeoDataFrame.from_features(result)\n",
    "        return geostats\n",
    "\n",
    "    ## make an ice velocity quiver plot\n",
    "\n",
    "#     fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "    df = salem.read_shapefile(pts_file) \n",
    "\n",
    "    df_file = loadshp_as_gpd(pts_file)\n",
    "    df_file.crs\n",
    "\n",
    "    coords = np.array([p.xy for p in df.geometry]).squeeze()\n",
    "    df['lon'] = coords[:, 0]\n",
    "    df['lat'] = coords[:, 1]\n",
    "\n",
    "    # ax.scatter(df.lon, df.lat, s=10, c='r' )#c='depth',cmap='viridis', s=10, ax=ax);\n",
    "    xx, yy = salem.transform_proj(salem.wgs84, grid.proj, df['lon'].values, df['lat'].values)\n",
    "    df['x'] = xx\n",
    "    df['y'] = yy\n",
    "\n",
    "    # shp_plt = reproject(df, VX)\n",
    "\n",
    "    X, Y = np.meshgrid(vx.coords['x'],vx.coords['y'])\n",
    "    U = vx.data[0]\n",
    "    V = vy.data[0]\n",
    "\n",
    "    # Q = ax.quiver(X, Y, U, V)#units='width', angles='uv') \n",
    "#     Q = ax.quiver(X[::20, ::20], Y[::20, ::20], U[::20, ::20], V[::20, ::20], units='xy')\n",
    "#     ax.set_xlim(502500, 507500)\n",
    "#     ax.set_ylim( 5625000, 5631500)\n",
    "#     #     plt.quiver(vx.x, vx.y, vx.data ,vy.data)\n",
    "#     plt.savefig(fl_path+ 'products/'+ glacier[gl]+ str(year)+'_quiver.pdf', dpi=300)\n",
    "\n",
    "    # grid.ij_coordinates\n",
    "    # v_crs = check_crs(vx)\n",
    "\n",
    "    vns = ['vx',\n",
    "           'vy',\n",
    "           'gpr',\n",
    "           'H_opt',\n",
    "           'dem',\n",
    "           'farin_corr',\n",
    "           #'msk',\n",
    "           ]\n",
    "\n",
    "    M = xr.merge([vx,vy,gpr,H_opt,dem,farin_corr]) #dem\n",
    "\n",
    "\n",
    "\n",
    "    for vn in vns:\n",
    "        df[vn] = M[vn][0].interp(x=('z', df.x), y=('z', df.y))\n",
    "\n",
    "\n",
    "\n",
    "    df_agg = df[['ID', 'len', 'distance', 'angle', 'geometry', 'lon', 'lat']].copy()\n",
    "    ii, jj = grid.transform(df['lon'], df['lat'], crs=salem.wgs84, nearest=True)\n",
    "    df_agg['i'] = ii\n",
    "    df_agg['j'] = jj\n",
    "    # # We trick by creating an index of similar i's and j's\n",
    "    # df_agg['ij'] = ['{:04d}_{:04d}'.format(i, j) for i, j in (vx.coords['x'], vx.coords['y'])]\n",
    "    df_agg['ij'] = ['{:04d}_{:04d}'.format(i, j) for i, j in zip(ii, jj)]\n",
    "    df_agg = df_agg.groupby('ij').mean()\n",
    "\n",
    "    # Select\n",
    "    for vn in vns:\n",
    "    #     M_maksed = np.ma.masked_invalid(M[vn][0])\n",
    "    #     df_agg[vn] = .interp(x=('z', df_agg.i), y=('z', df_agg.j))\n",
    "        df_agg[vn] = M[vn][0].isel(x=('z', df_agg.i), y=('z', df_agg.j))  \n",
    "\n",
    "    D = df_agg[['ID', 'len', 'distance', 'angle', 'lon', 'lat', 'vx', 'vy', 'gpr', 'H_opt', 'dem', 'farin_corr']].copy()\n",
    "\n",
    "\n",
    "    import math\n",
    "    D['vz'] = np.sqrt(D.vx**2 + D.vy**2 )  # np.sqrt(U**2 + V**2 )\n",
    "\n",
    "    D['vzdir'] = np.arcsin(D.vx/D.vz) *180/math.pi #degrees from north\n",
    "    # D.vzdir[D.vzdir>25.0]=np.nan  ##remove outliers for Kokanee only!!\n",
    "\n",
    "    ###this step for Illec only!!\n",
    "    # D.vzdir[D.vzdir>0.0]=np.nan ## remove false velocity vectors which plague the accumulation zone \n",
    "\n",
    "\n",
    "    #multiply velocity vector by cosine of angle between vector and flux gate (line or line segment)\n",
    "    D['vfg'] = np.abs(D.vz * np.cos((D.vzdir-(D.angle-90.))*(math.pi/180.))) #velocity normal to gate per slice\n",
    "\n",
    "    # D[D.ID==0].Qopt = D.vfg * 1.0 * 20. * D.H_opt\n",
    "    # D[D.ID==1].Qopt = D.vfg * 1.0 * 20. * D.H_opt\n",
    "\n",
    "#     KP=[]\n",
    "#     KF=[]\n",
    "#     # Ice flux where lowest two bins have sliding velocity equal to surface velocity\n",
    "#     for loop in D.index:\n",
    "#         ID= D[D.index==loop].ID.values\n",
    "#     #     if ((ID==0) | (ID==1)):     \n",
    "#     #         KP.append( ((D[D.index==loop].vfg - off_ice_V)* (Dint+0.1) * d * D.H_opt[D.index==loop]).values[0])  \n",
    "#     #         KF.append( ((D[D.index==loop].vfg - off_ice_V)* (Dint+0.1) * d * D.farin_corr[D.index==loop]).values[0])\n",
    "#     #     else:\n",
    "#         KP.append((D[D.index==loop].vfg * Dint * d * D.H_opt[D.index==loop]).values[0])  ## ice flux per slice *0.9 to est. depth-integrated velocity\n",
    "#         KF.append((D[D.index==loop].vfg * Dint * d * D.farin_corr[D.index==loop]).values[0])\n",
    "#     D['Qopt'] = KP\n",
    "#     D['Qfarin'] = KF\n",
    "\n",
    "    if vcorr==True:\n",
    "        D['Qopt'] = (D.vfg - off_ice_V + sVZ) * Dint * d * D.H_opt \n",
    "        D['Qfarin'] = (D.vfg - off_ice_V + sVZ) * Dint * d * D.farin_corr \n",
    "        ## ice flux per slice *0.9 to est. depth-integrated velocity\n",
    "    else:\n",
    "        D['Qopt'] = (D.vfg) * Dint * d * D.H_opt \n",
    "        D['Qfarin'] = (D.vfg) * Dint * d * D.farin_corr \n",
    "\n",
    "    D['a'] = d * D.H_opt #area per slice\n",
    "    D['a_farin'] = d * D.farin_corr #area per slice\n",
    "\n",
    "    ### Uncertainties\n",
    "    # D['sQopt'] = np.sqrt((sVZ**2) * ((D.H_opt**2 + (sHopt * D.H_opt)**2) * (D.vfg*0.9)**2) * d**2) #d=10 meters, point spacing\n",
    "    D['sQout'] = np.sqrt((sVZ * (D.H_opt) * d)**2 + ((D.vfg*Dint) * d * (sHopt*D.H_opt))**2)\n",
    "    D['sQoutF'] = np.sqrt((sVZ * (D.farin_corr) * d)**2 + ((D.vfg*Dint) * d * (sHopt*D.farin_corr))**2)\n",
    "\n",
    "    # D['sQout'] = np.sqrt((sVZ**2) * ((D.H_opt**2 + (sHopt*D.H_opt)**2) * (D.vfg*Dint)**2) * d**2)\n",
    "    # D['sQoutF'] = np.sqrt((sVZ**2) * ((D.farin_corr**2 + (sHopt*D.farin_corr)**2) * (D.vfg*Dint)**2) * d**2)\n",
    "\n",
    "\n",
    "\n",
    "    Q_out= [0.,] #flux out per gate\n",
    "    cr_area = [0.,] # cross-section area per gate\n",
    "    vzdir = [0.,]\n",
    "    sQout= [0.,]\n",
    "\n",
    "\n",
    "    for n in range(D.ID.nunique()+1):\n",
    "            Q_out.append(D.Qopt[D.ID==n].sum()) \n",
    "            cr_area.append(D.a[D.ID==n].sum())\n",
    "            vzdir.append(D.vzdir[D.ID==n].mean())\n",
    "            sQout.append(D.sQout[D.ID==n].sum()) \n",
    "    #         area_Qin.append(D.[D.ID==n].sum()) \n",
    "    Q_in = [Q_out[1:]]\n",
    "    sQin = [sQout[1:]+ [0.0]]\n",
    "    # area_Qin = \n",
    "    # area_Qin= \n",
    "    Q_in = np.squeeze(Q_in)\n",
    "    sQin = np.squeeze(sQin)\n",
    "\n",
    "\n",
    "    Q_outF= [0.,] #flux out per gate\n",
    "    cr_areaF = [0.,]\n",
    "    sQoutF = [0.,]\n",
    "    for n in range(D.ID.nunique()+1):\n",
    "            Q_outF.append(D.Qfarin[D.ID==n].sum()) \n",
    "            cr_areaF.append(D.a_farin[D.ID==n].sum()) \n",
    "            sQoutF.append(D.sQoutF[D.ID==n].sum())\n",
    "\n",
    "    Q_inF = [Q_outF[1:]] #flux in per bin\n",
    "    Q_inF = np.squeeze(Q_inF)\n",
    "    sQinF = [sQoutF[1:]+ [0.0]]\n",
    "    sQinF = np.squeeze(sQinF)\n",
    "\n",
    "    FG = pd.DataFrame(list(zip(Q_out, Q_in, cr_area, sQout, sQin, Q_outF, Q_inF, sQoutF, sQinF, cr_areaF, vzdir)), \n",
    "            columns=['Q_out', 'Q_in', 'cr_area', 'sQout', 'sQin', 'Q_outF', 'Q_inF','sQoutF', 'sQinF', 'cr_areaF', 'vzdir']) \n",
    "\n",
    "\n",
    "    FG.loc[cb, 'Q_in'] = FG.Q_out[cb+1]+ FG.Q_out[cb+2] \n",
    "    FG.loc[cb, 'Q_inF'] = FG.Q_outF[cb+1]+ FG.Q_outF[cb+2] \n",
    "\n",
    "    FG['vel_fg'] = FG.Q_out / FG.cr_area #net velocity per gate\n",
    "    FG['vel_fgF'] = FG.Q_outF / FG.cr_area\n",
    "    FG['bin']=np.arange(0,len(range(D.ID.nunique()+1)),1)\n",
    "    # FG['sQnet_opt'] = np.sqrt(FG.sQopt**2 + FG.sQin**2)\n",
    "\n",
    "    FG['spQout'] = FG.sQout / FG.Q_out * 100. #%err on flux\n",
    "    FG['spQin'] = FG.sQin / FG.Q_in * 100. #%err on flux\n",
    "    FG['spQoutF'] = FG.sQoutF / FG.Q_outF * 100. #%err on flux\n",
    "    FG['spQinF'] = FG.sQinF / FG.Q_inF * 100. #%err on flux\n",
    "\n",
    "    ### import data per bin: height change, elevation, surface area, obs.SMB etc. \n",
    "\n",
    "    FG_df = FG.copy()\n",
    "    dem_r = topo\n",
    "    vel_r = VY #vdir + vf[:-1] + '-F_vm.tif'\n",
    "    shp = loadshp_as_gpd(bins)\n",
    "\n",
    "    rasters = [dh_r, dem_r, vel_r]\n",
    "    names = ['dh','dem','vy']\n",
    "    for i in range(len(rasters)):\n",
    "        raster = rasters[i]\n",
    "        rst = rasterio.open(raster)\n",
    "        shp = reproject(shp, rst) #shp is in correct projection, trying anyway for calculation\n",
    "        # dhdt= vx.salem.transform(dhdt)\n",
    "        name = names[i]\n",
    "    #     shp_vec = dissolve_gpd(shp)  ## only if more columns exist\n",
    "        stat = stats_select()  #'['min', 'max', 'mean', 'count', 'sum', 'std', 'median', 'majority', 'minority', 'unique', 'range']\n",
    "        ZS = (get_zonal_stats(shp, raster, stat))\n",
    "        ZS.drop(['geometry'], axis=1, inplace=True)\n",
    "        ZS.rename(columns={\"mean\": name+\"_mean\", \"median\":name+\"_med\", \"std\": name+\"_std\", \"count\":name+\"_count\"}, inplace=True)\n",
    "        ZS.sort_values(by=['bin'],ascending=True, inplace=True)\n",
    "        ZS.set_index('bin', inplace=True)\n",
    "        FG_df =  pd.concat([FG_df, ZS], axis=1)\n",
    "\n",
    "\n",
    "    rho.sort_values(by='bin',ascending=True, inplace=True) # sort by bin\n",
    "    rho.set_index('bin', inplace=True) # set bin as index for sorting\n",
    "    rho[0:4]\n",
    "    FG_df =  pd.concat([FG_df, rho], axis=1)\n",
    "\n",
    "    WR= np.array(FG_df.dem_count[1:])\n",
    "    WR= np.append(WR,[0])\n",
    "\n",
    "    FG_df.loc[(cb+1), 'Q_in'] = 0.0  ## set Q_in to zero for top of west wing \n",
    "\n",
    "    FG_df['Q_net'] = FG_df.Q_in - FG_df.Q_out  #net flux per gate\n",
    "    FG_df.loc[cb, 'Q_net'] = FG_df.Q_out[cb+1]+ FG_df.Q_out[cb+2] - FG_df.Q_out[cb] # flux for middle bin\n",
    "    FG_df['Q_netA']= FG_df.Q_net / FG_df.dem_count\n",
    "\n",
    "    FG_df['Q_netF'] = FG_df.Q_inF - FG_df.Q_outF  #net flux per gate\n",
    "    FG_df.loc[cb, 'Q_netF'] = FG_df.Q_outF[cb+1]+ FG_df.Q_outF[cb+2] - FG_df.Q_outF[cb] # flux for middle bin\n",
    "    FG_df['Q_netAF']= FG_df.Q_netF / FG_df.dem_count\n",
    "\n",
    "    FG_df['area_Qin']= WR\n",
    "    FG_df.loc[cb, 'area_Qin'] = FG_df.dem_count[cb+1]+ FG_df.dem_count[cb+2]\n",
    "\n",
    "    FG_df['sQoptA'] = FG_df.sQout / FG_df.dem_count\n",
    "    FG_df['sQ_inA'] = FG_df.sQin / (FG_df.area_Qin+0.001)\n",
    "    FG_df['sQnetA_opt'] = np.sqrt(FG_df.sQoptA**2 + FG_df.sQ_inA**2)\n",
    "\n",
    "    FG_df['sQoutFA'] = FG_df.sQoutF / FG_df.dem_count\n",
    "    FG_df['sQ_inFA'] = FG_df.sQinF / (FG_df.area_Qin+0.001)\n",
    "    FG_df['sQnetFA'] = np.sqrt(FG_df.sQoutFA**2 + FG_df.sQ_inFA**2)\n",
    "\n",
    "\n",
    "    ##calculate height change due to mass balance for highest bins considering firn compaction\n",
    "    FG_df['b_fg_h'] = FG_df.dh_mean - FG_df.Q_netA + FG_df.Vfirn\n",
    "    FG_df['b_fg_hF'] = FG_df.dh_mean - FG_df.Q_netAF + FG_df.Vfirn\n",
    "\n",
    "    # FG_df['b_fg_h'] = FG_df.dh_mean - FG_df.Q_netA\n",
    "    ## Ilec slope, constant: (0.025293682808396056, 400.09590745271896)\n",
    "    FG_df['rho_snow'] = (FG_df.dem_mean*(-0.02529))+400.095  #*-0.11255+743.22)\n",
    "    if balance == 'Bw':\n",
    "    #     FG_df['b_fg_we'] = rho_snow/1000. * FG_df.b_fg_h\n",
    "        FG_df['b_fg_we'] = FG_df.rho_snow/1000 * FG_df.b_fg_h\n",
    "    else:\n",
    "        FG_df['b_fg_we'] = FG_df['rho_%s'%year]/1000. * FG_df.b_fg_h\n",
    "\n",
    "    if balance == 'Bw':\n",
    "    #     FG_df['b_fg_weF'] = rho_snow/1000. * FG_df.b_fg_hF\n",
    "        FG_df['b_fg_weF'] = FG_df.rho_snow/1000 * FG_df.b_fg_hF\n",
    "    else:\n",
    "        FG_df['b_fg_weF'] = FG_df['rho_%s'%year]/1000. * FG_df.b_fg_hF\n",
    "    # FG_df.drop(FG_df.tail(1).index,inplace=True)\n",
    "\n",
    "    ########## test mass conservation: emergence vel + thinning = b_dot\n",
    "    if balance == 'Bw':\n",
    "        FG_df['Bmass_con'] = (FG_df.Q_netA * FG_df.rho_snow/1000) + (FG_df.dh_mean * FG_df.rho_snow/1000) \n",
    "    else:\n",
    "        FG_df['Bmass_con'] = (FG_df.Q_netA * FG_df['rho_%s'%year]/1000) + (FG_df.dh_mean * FG_df['rho_%s'%year]/1000)  \n",
    "\n",
    "    if balance == 'Bw':\n",
    "        FG_df['BFmass_con'] = (FG_df.Q_netAF * FG_df.rho_snow/1000) + (FG_df.dh_mean * FG_df.rho_snow/1000) \n",
    "    else:\n",
    "        FG_df['BFmass_con'] = (FG_df.Q_netAF * FG_df['rho_%s'%year]/1000) + (FG_df.dh_mean * FG_df['rho_%s'%year]/1000)\n",
    "\n",
    "    ### Uncertainties\n",
    "    sDHdt = 0.04 #m Bias dh from Pelto et al. 2019\n",
    "    sVfirn = 0.10\n",
    "    sRHO = 0.05 # percent uncertainty in density\n",
    "\n",
    "    # if FG_df.Vfirn > 0.00:\n",
    "    # FG_df['sDH_opt'] = np.sqrt(sDHdt**2 + (FG_df.sQnet_opt / FG_df.dem_count)**2 + (FG_df.Vfirn*sVfirn)**2) #FG_df.sQnet_opt / FG_df.dem_count\n",
    "    FG_df['sDH_opt'] = np.sqrt(sDHdt**2 + (FG_df.sQnetA_opt)**2 + (FG_df.Vfirn*sVfirn)**2)\n",
    "    FG_df['sBwe_opt'] = np.sqrt((FG_df.sDH_opt * (FG_df['rho_%s'%year]/1000.))**2+\n",
    "                   (FG_df.dh_mean * (FG_df['rho_%s'%year]/1000.*sRHO))**2)\n",
    "\n",
    "    FG_df['sDH_F'] = np.sqrt(sDHdt**2 + (FG_df.sQnetFA)**2 + (FG_df.Vfirn*sVfirn)**2)\n",
    "    FG_df['sBwe_F'] = np.sqrt((FG_df.sDH_F * (FG_df['rho_%s'%year]/1000.))**2+\n",
    "                   (FG_df.dh_mean * (FG_df['rho_%s'%year]/1000.*sRHO))**2)\n",
    "\n",
    "    D['Agpr']=np.NaN\n",
    "    D.sort_values(by=['distance'],ascending=True, inplace=True) \n",
    "    D = D.dropna(subset=['gpr'])\n",
    "    for n in range(D.ID.nunique()):\n",
    "\n",
    "        for c in range(D.ID[D.ID==n].count()-1):\n",
    "            idx=D[(D.ID==n)&(D.distance==D[D.ID==n].distance[c])].index\n",
    "\n",
    "            if c==0:\n",
    "                G= D.distance[D.ID==n][c+1] - D.distance[D.ID==n][c] * D.gpr[D.ID==n][c+1]*0.5 #area of triangle\n",
    "            elif c==1:\n",
    "                G= (D.distance[D.ID==n][c+1] - D.distance[D.ID==n][c])/2 * D.gpr[D.ID==n][c]\n",
    "            elif c==len(D.ID[D.ID==n])-2: #\n",
    "                G= (D.distance[D.ID==n][c] - D.distance[D.ID==n][c-1])/2 * D.gpr[D.ID==n]\n",
    "            elif c==len(D.ID[D.ID==n])-1:  #last point in line\n",
    "                G= D.distance[D.ID==n][c] + D.distance[D.ID==n][c+1] * D.gpr[D.ID==n][c-1]*0.5 #area of triangle\n",
    "            else:\n",
    "                G=(((D.distance[D.ID==n][c+1]-D.distance[D.ID==n][c])/2) + ((D.distance[D.ID==n][c] -\n",
    "                        D.distance[D.ID==n][c-1])/2))* D.gpr[D.ID==n][c]  \n",
    "\n",
    "            D.loc[idx,'Agpr']= G\n",
    "    #         D['Q_gpr'] = (D.vfg * 0.9 *D['Agpr'])\n",
    "    #     D['A'] = D.vfg[D.ID==n] * D.gpr[D.ID==n]\n",
    "    for n in range(D.ID.nunique()):\n",
    "\n",
    "        for c in range(D.ID[D.ID==n].count()-1):\n",
    "            idx=D[(D.ID==n)&(D.distance==D[D.ID==n].distance[c])].index\n",
    "\n",
    "            if c==0:\n",
    "                L= D.distance[D.ID==n][c+1] - D.distance[D.ID==n][c] \n",
    "            elif c==1:\n",
    "                L= (D.distance[D.ID==n][c+1] - D.distance[D.ID==n][c])/2 \n",
    "            elif c==len(D.ID[D.ID==n])-2: #\n",
    "                L= (D.distance[D.ID==n][c] - D.distance[D.ID==n][c-1])/2 \n",
    "            elif c==len(D.ID[D.ID==n])-1:  #last point in line\n",
    "                L= D.distance[D.ID==n][c] + D.distance[D.ID==n][c+1] \n",
    "            else:\n",
    "                L=(((D.distance[D.ID==n][c+1]-D.distance[D.ID==n][c])/2) + ((D.distance[D.ID==n][c] -\n",
    "                        D.distance[D.ID==n][c-1])/2))\n",
    "\n",
    "            D.loc[idx,'Len_gpr']= L\n",
    "\n",
    "    #calculate flux out using GPR thicknesses\n",
    "    sGPR = 0.0516 #gpr error %\n",
    "    D['sInterp_gpr'] = np.nan\n",
    "    for n in range(len(D.Len_gpr)):\n",
    "        if D.Len_gpr[n] > 50.:\n",
    "            D.sInterp_gpr[n] = D.gpr[n] * 0.10  \n",
    "        else: \n",
    "            D.sInterp_gpr[n] = 0.0\n",
    "    sH_gpr = np.sqrt(D.sInterp_gpr**2+ (D.gpr*sGPR)**2)\n",
    "    if vcorr==True:\n",
    "        D['Qo_gpr'] = D.Agpr * (D.vfg - off_ice_V  + sVZ) * Dint#* 0.9\n",
    "    else:\n",
    "        D['Qo_gpr'] = D.Agpr * (D.vfg) * Dint#* 0.9\n",
    "    D['sQo_gpr'] =  np.sqrt((sVZ * (D.gpr) * D.Len_gpr)**2 + ((D.vfg*Dint) * D.Len_gpr * (sH_gpr))**2)\n",
    "\n",
    "    # D['sQo_gpr'] = np.sqrt((sVZ**2) * ((D.gpr**2 + (sH_gpr)**2) * (D.vfg*Dint)**2) * d**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Qo_gpr = [0.,] #flux out per gate\n",
    "    cr_area_gpr = [0.,] # cross-section area per gate\n",
    "    vzdir_gpr = [0.,]\n",
    "    sQo_gpr= [0.,]\n",
    "    n=0\n",
    "    for n in range(D.ID.nunique()):\n",
    "            Qo_gpr.append(D.Qo_gpr[D.ID==n].sum()) \n",
    "            cr_area_gpr.append(D.Agpr[D.ID==n].sum())\n",
    "            vzdir_gpr.append(D.vzdir[D.ID==n].mean())\n",
    "            sQo_gpr.append(D.sQo_gpr[D.ID==n].sum())\n",
    "    Qin_gpr = [Qo_gpr[1:]+ [0.0]] #flux in per bin, add zero to end for top bin (for now)\n",
    "    Qin_gpr = np.squeeze(Qin_gpr)\n",
    "    sQin_gpr = [sQo_gpr[1:]+ [0.0]]\n",
    "    sQin_gpr = np.squeeze(sQin_gpr)\n",
    "\n",
    "    FG_gpr = pd.DataFrame(list(zip(Qo_gpr, Qin_gpr, cr_area_gpr, sQo_gpr,sQin_gpr)), \n",
    "                          columns=['Qo_gpr', 'Qin_gpr', 'cr_area_gpr', 'sQo_gpr','sQin_gpr']) \n",
    "    FG_gpr['bin'] = np.arange(0,len(range(D.ID.nunique()+1)),1)#[0,1,2,3]\n",
    "    # FG_gpr['vfg'] = FG_gpr.Qo_gpr / FG_gpr.cr_area_gpr #net velocity per gate\n",
    "    # FG['bin']=np.arange(0,14,1)\n",
    "\n",
    "\n",
    "    FG_gpr.loc[cb+1, 'Qo_gpr'] = FG_df.Q_out[cb+1] #flux out for bin 8, which has no radar measurements\n",
    "    FG_gpr.loc[cb, 'Qin_gpr'] = FG_gpr.Qo_gpr[cb+1]+ FG_gpr.Qo_gpr[cb+2] #flux in for center bin\n",
    "\n",
    "    FG_gpr['spQo_gpr'] = FG_gpr.sQo_gpr / FG_gpr.Qo_gpr * 100. #%err on flux\n",
    "    FG_gpr['spQin_gpr'] = FG_gpr.sQin_gpr / FG_gpr.Qin_gpr * 100. #%err on flux\n",
    "\n",
    "    FG_all = FG_df.merge(FG_gpr, how='left') #on='bin')  \n",
    "    # FG_all\n",
    "    FG_all['Q_net_gpr'] = FG_all.Qin_gpr - FG_all.Qo_gpr \n",
    "\n",
    "\n",
    "    # FG_all.loc[cb, 'Q_net_gpr'] = FG_all.Q_out[cb+1]+ FG_all.Qo_gpr[cb+2] - FG_all.Qo_gpr[cb]\n",
    "    # FG_all.loc[14, 'Q_net_gpr'] = FG_all.Q_out[14]\n",
    "    # FG_all.loc[14, 'sQo_gpr'] = FG_df.sQout[14]\n",
    "    # FG_all.loc[14, 'sQin_gpr'] = FG_df.sQin[14]\n",
    "\n",
    "    FG_all['Q_netA_gpr']= FG_all.Q_net_gpr / FG_all.dem_count \n",
    "    FG_all['b_fg_h_gpr'] = FG_all.dh_mean - FG_all.Q_netA_gpr  #dh1516_mean\n",
    "\n",
    "\n",
    "    FG_all['sQoutA_gpr'] = FG_all.sQo_gpr / FG_all.dem_count\n",
    "    FG_all['sQ_inA_gpr'] = FG_all.sQin_gpr / (FG_all.area_Qin+0.001)\n",
    "    FG_all['sQnetA_gpr'] = np.sqrt(FG_all.sQoutA_gpr**2 + FG_all.sQ_inA_gpr**2)\n",
    "\n",
    "\n",
    "    if balance == 'Bw':\n",
    "        FG_all['b_fg_we_gpr'] = FG_df.rho_snow/1000. * FG_all.b_fg_h_gpr\n",
    "    else:\n",
    "        FG_all['b_fg_we_gpr'] = FG_all['rho_%s'%year]/1000. * FG_all.b_fg_h_gpr\n",
    "    # FG_all.drop(FG_all.tail(1).index,inplace=True)\n",
    "\n",
    "    FG_all['sDH_gpr'] = np.sqrt(sDHdt**2 + (FG_all.sQnetA_gpr)**2 + (FG_all.Vfirn*sVfirn)**2)\n",
    "    FG_all['sBwe_gpr'] = np.sqrt((FG_all.sDH_gpr * (FG_all['rho_%s'%year]/1000.))**2+\n",
    "                   (FG_all.dh_mean * (FG_all['rho_%s'%year]/1000.*sRHO))**2)\n",
    "\n",
    "\n",
    "    ###########test mass conservation ####\n",
    "    if balance == 'Bw':\n",
    "        FG_all['Bgpr_mass_con'] = (FG_all.Q_netA_gpr * FG_df.rho_snow/1000) + (FG_all.dh_mean * FG_df.rho_snow/1000) \n",
    "    else:\n",
    "        FG_all['Bgpr_mass_con'] = (FG_all.Q_netA_gpr * FG_df['rho_%s'%year]/1000) + (FG_all.dh_mean * FG_df['rho_%s'%year]/1000)  \n",
    "\n",
    "\n",
    "################# start split grad fig #######################################################\n",
    "    # fig, ax = plt.subplots(1, sharex=True, sharey=True, figsize=(6,3.34))#3.34, 3.34))\n",
    "    # # j,k=0,0    ## only define j if one row\n",
    "    # n = 0\n",
    "    # s= 15 #markersize\n",
    "\n",
    "    # a = 0.9\n",
    "    # color=['b', 'lime', 'green', 'teal']\n",
    "\n",
    "    # ##all_obs\n",
    "    # if balance == 'Bw':\n",
    "    #     obs = obs.dropna(subset=['Bw'])\n",
    "    #     y_ax_obs=obs[(obs.Year==year)].Bw\n",
    "    #     y_acc_obs=obs[(obs.Year==year)&(obs.Elev>2470.)].Bw\n",
    "    #     y_abl_obs=obs[(obs.Year==year)&(obs.Elev<2470.)].Bw\n",
    "\n",
    "    # else:\n",
    "    #     obs = obs.dropna(subset=['Ba'])\n",
    "    #     y_ax_obs=obs[(obs.Year==year)].Ba\n",
    "    #     y_acc_obs=obs[(obs.Year==year)&(obs.Elev>2470.)].Ba\n",
    "    #     y_abl_obs=obs[(obs.Year==year)&(obs.Elev<2470.)].Ba\n",
    "\n",
    "    # x_ax_obs=obs[(obs.Year==year)].Elev\n",
    "    # x_acc_obs=obs[(obs.Year==year)&(obs.Elev>2470.)].Elev\n",
    "    # x_abl_obs=obs[(obs.Year==year)&(obs.Elev<2470.)].Elev\n",
    "\n",
    "\n",
    "    # ##obs by bin\n",
    "    # # x_ax_obs=FG_df._elevmean  #FG_df.elev_mean\n",
    "    # # y_ax_obs=FG_df.b_obs16#b_obs\n",
    "\n",
    "    # # Drop unrealistic FG smb:\n",
    "    # new = FG_all.copy()\n",
    "\n",
    "    # new = new[new.b_fg_we >-8.]\n",
    "    # # new = new[new.b_fg_we_gpr >-8.]\n",
    "\n",
    "    # #errors (above and below)\n",
    "    # # new['yerr'] = 0.20\n",
    "    # yerr = [0.0, new.sBwe_gpr[new.dem_mean>2470] , new.sBwe_opt[new.dem_mean>2470], new.sBwe_F[new.dem_mean>2470],\n",
    "    #         0.0, new.sBwe_gpr[new.dem_mean<2470] , new.sBwe_opt[new.dem_mean<2470], new.sBwe_F[new.dem_mean<2470]] #new.sBwe_gpr\n",
    "    # new['xerr'] = new.dem_std * 1.5 #2 std dev -- 95% of data\n",
    "    # xerr = new.xerr \n",
    "\n",
    "    # ### ALL points ###\n",
    "    # x_ax_fg_gpr=new.dem_mean\n",
    "    # y_ax_fg_gpr=new.b_fg_we_gpr\n",
    "\n",
    "    # x_ax_fg_opt=new.dem_mean  #FG_df.elev_mean\n",
    "    # y_ax_fg_opt=new.b_fg_we\n",
    "\n",
    "    # x_ax_fg_F=new.dem_mean  \n",
    "    # y_ax_fg_F=new.b_fg_weF\n",
    "\n",
    "    # # x = [x_ax_obs, x_ax_fg_gpr, x_ax_fg_opt, x_ax_fg_F]\n",
    "    # # y = [y_ax_obs, y_ax_fg_gpr, y_ax_fg_opt, y_ax_fg_F]\n",
    "\n",
    "    # ###ACC zone ####\n",
    "    # x_acc_fg_gpr=new.dem_mean[new.dem_mean>2470]\n",
    "    # y_acc_fg_gpr=new.b_fg_we_gpr[new.dem_mean>2470]\n",
    "\n",
    "    # x_acc_fg_opt=new.dem_mean[new.dem_mean>2470]  #FG_df.elev_mean\n",
    "    # y_acc_fg_opt=new.b_fg_we[new.dem_mean>2470]\n",
    "\n",
    "    # x_acc_fg_F=new.dem_mean[new.dem_mean>2470]  \n",
    "    # y_acc_fg_F=new.b_fg_weF[new.dem_mean>2470]\n",
    "\n",
    "    # ### ABL zone ###\n",
    "    # x_abl_fg_gpr=new.dem_mean[new.dem_mean<2470]\n",
    "    # y_abl_fg_gpr=new.b_fg_we_gpr[new.dem_mean<2470]\n",
    "\n",
    "    # x_abl_fg_opt=new.dem_mean[new.dem_mean<2470]  #FG_df.elev_mean\n",
    "    # y_abl_fg_opt=new.b_fg_we[new.dem_mean<2470]\n",
    "\n",
    "    # x_abl_fg_F=new.dem_mean[new.dem_mean<2470]  \n",
    "    # y_abl_fg_F=new.b_fg_weF[new.dem_mean<2470]\n",
    "\n",
    "\n",
    "    # # x = [x_ax_obs, x_ax_fg_gpr, x_ax_fg_opt, x_ax_fg_F, x_acc_obs, x_acc_fg_gpr, x_acc_fg_opt, x_acc_fg_F, x_abl_obs, x_abl_fg_gpr, x_abl_fg_opt, x_abl_fg_F]\n",
    "    # # y = [y_ax_obs, y_ax_fg_gpr, y_ax_fg_opt, y_ax_fg_F, y_acc_obs, y_acc_fg_gpr, y_acc_fg_opt, y_acc_fg_F, y_abl_obs, y_abl_fg_gpr, y_abl_fg_opt, y_abl_fg_F]\n",
    "    # x = [x_acc_obs, x_acc_fg_gpr, x_acc_fg_opt, x_acc_fg_F, x_abl_obs, x_abl_fg_gpr, x_abl_fg_opt, x_abl_fg_F]\n",
    "    # y = [y_acc_obs, y_acc_fg_gpr, y_acc_fg_opt, y_acc_fg_F, y_abl_obs, y_abl_fg_gpr, y_abl_fg_opt, y_abl_fg_F]\n",
    "\n",
    "    # color = ['k', 'teal', '#74c476', '#238b45', 'k', 'teal', '#74c476', '#238b45']\n",
    "    # label = ['Obs.', 'Fg gpr', 'Fg opt.','Farinotti','','','','']\n",
    "    # sym = ['o', '^', 's', 'd','o', '^', 's', 'd']\n",
    "    # # Call function to create error bars \n",
    "    # shift = [-25,0,25,0,-25,0,25]\n",
    "    # for i in [0,1,2,4,5,6]:#range(6):\n",
    "    #     ax.errorbar((x[i+1]+shift[i]), y[i+1], xerr=None, yerr=yerr[i+1], fmt=sym[i+1], ecolor=color[i+1], zorder=2,\n",
    "    #                           label=label[i+1], alpha=0.4, c=color[i+1]) #elinewidth=0.7\n",
    "    # #     _ = make_error_boxes(ax, x[i+1], y[i+1], xerr, yerr, ecolor=color[i+1])  #[0]\n",
    "    # #     _ = make_error_boxes(ax, x[i+1], y[i+1], xerr, yerr, hue=color[i+1])[1]\n",
    "\n",
    "    # ## create range bar to represent flux gate elevation range\n",
    "    # # theives = ax.errorbar(x[2], y[2], xerr=xerr[2], yerr=None, fmt='None', ecolor=color[2], zorder=1,\n",
    "    # #                           elinewidth=8, marker='d', label='Fg range', ls='..', alpha=0.25)\n",
    "\n",
    "    # ## plot data and regression lines\n",
    "    # ax.scatter(x_ax_obs,y_ax_obs,color=color[0], label=label[0], alpha=a, s=s,facecolor='', zorder=3)  ##plot obs data x[0],y[0]\n",
    "\n",
    "    # # ###plot mass con\n",
    "    # # MC_y = [FG_all.Bgpr_mass_con, FG_all.Bmass_con, FG_all.BFmass_con]\n",
    "    # # for i in range(3):\n",
    "    # #     ax.errorbar(FG_all.dem_mean+shift[i],MC_y[i], xerr=None, yerr=FG_all.sBwe_opt, fmt=sym[i+1], c='r', label='', \n",
    "    # #             alpha=a, zorder=3)\n",
    "\n",
    "    # ytxt = [0.2, 0.15, 0.1, 0.05]\n",
    "    # for i in range(8):\n",
    "    # #     fit = np.polyfit(x[i],y[i],2)\n",
    "    # #     y_new = np.polyval(fit,x[i])\n",
    "    # #     ax.plot(x[i],y[i],c=color[i], linewidth=1.0, linestyle='--', alpha=a-0.1)\n",
    "\n",
    "    #     slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(x[i]),np.array(y[i]))\n",
    "    # #     ax.scatter(x[i],y[i],color=color[i], label=label[i], alpha=a, s=s,facecolor='', zorder=3)\n",
    "\n",
    "    # #     ax = sns.boxplot(x=x[i], y=y[i], palette='copper', saturation=0.7)#, ax=ax)\n",
    "    # #     ax.plot(x[i],slope*x[i]+intercept, color=color[i],label='Lin', alpha=a, linewidth=0.9, zorder=3)\n",
    "    # #     print(slope, intercept, r_value, p_value, std_err)\n",
    "    #     txt= ['Obs. slope:        ', 'FG gpr slope:    ', 'FG opt. slope:   ', 'FG Farin. slope: ']\n",
    "\n",
    "    # #     'H_{opt}''slope:$'\n",
    "\n",
    "    #     #################### RLM  https://www.statsmodels.org/stable/rlm.html\n",
    "    #     ###https://www.statsmodels.org/devel/generated/statsmodels.robust.robust_linear_model.RLMResults.html\n",
    "    #     exog = stm.add_constant(x[i]) #obs_data.Elev)\n",
    "    #     rlm_model = stm.RLM(y[i],exog, M=stm.robust.norms.HuberT()) #data.endog, data.exog,\n",
    "    #     rlm_results = rlm_model.fit()\n",
    "    #     stdE=rlm_results.bse[1]\n",
    "    #     # print(rlm_results.params)\n",
    "    #      # ax.scatter(x[i],y[i],color=color[i], label='data', alpha=a, s=s,facecolor='', zorder=3)\n",
    "    #     #     ax = sns.boxplot(x=x[i], y=y[i], palette='copper', saturation=0.7)#, ax=ax)\n",
    "    #     ax.plot(x[i],rlm_results.params[1]*x[i]+rlm_results.params[0], color=color[i],label='', alpha=a, \n",
    "    #             linewidth=0.8, zorder=3) # linestyle='--'\n",
    "    #     print('slope RLM:', rlm_results.params[1])\n",
    "    #     #############################\n",
    "    # #     ax.text(0.7, ytxt[i], txt[i]+str(np.round(1000*rlm_results.params[1],2))+ ' +/- ' + \n",
    "    # #             str(np.round(1000*stdE,2)), transform=ax.transAxes)\n",
    "\n",
    "    # if balance == 'Ba':\n",
    "    #     ax.axhline(linewidth=1, color='k', ls='--', alpha=a-0.2, zorder=0)\n",
    "    # # ax.axvline(2475.,linewidth=1, color='k', ls='--', alpha=a-0.2, zorder=0)\n",
    "    # ############ obs data box plot ################\n",
    "    # obs_data=obs[(obs.Year==year)]\n",
    "    # obs_data.reset_index(inplace=True)\n",
    "\n",
    "    # bin_size = 100.\n",
    "    # z_range = np.arange(1850., 3350., bin_size)\n",
    "    # # print(z_range)\n",
    "    # # obs_data['Elev_bin'] = np.nan\n",
    "\n",
    "    # i = 0\n",
    "    # # for n in range(len(obs_data.Ba)):\n",
    "\n",
    "    # Z_F = []\n",
    "    # Zor_F= []\n",
    "    # OB_F=[]\n",
    "\n",
    "    # for z in z_range: \n",
    "    #     OBS = []\n",
    "    #     Z = []\n",
    "    #     Zor = []\n",
    "    #     W=0\n",
    "    #     bin_size = 100.\n",
    "    #     while W==0:\n",
    "    #         for n in range(len(obs_data[balance])):\n",
    "    #             if ((z - bin_size/2.) <= obs_data.Elev[n]) and (obs_data.Elev[n] <=  (z + bin_size/2.)):\n",
    "    #                 O = obs_data[balance][n]\n",
    "    #                 Z.append(z)\n",
    "    #                 Zor.append(obs_data.Elev[n])\n",
    "    #                 OBS.append(O)\n",
    "\n",
    "    #         if len(OBS)<3:\n",
    "    #             ##could select last element below elevation\n",
    "    #             bin_size=bin_size*1.25\n",
    "    #         else:\n",
    "    #             OB_F.append(np.array(OBS))\n",
    "    #             Z_F.append(np.array(Z))\n",
    "    #             Zor_F.append(np.array(Zor))\n",
    "\n",
    "    #             W=1\n",
    "    #     i += 1\n",
    "    # # OB_F=np.concatenate(OB_F)\n",
    "    # # Z_F=np.concatenate(Z_F)\n",
    "    # # Zor_F=np.concatenate(Zor_F)\n",
    "    # # print(Z_F, len(Z_F))\n",
    "    # # print(OB_F, len(OB_F))\n",
    "\n",
    "\n",
    "\n",
    "    # # thieves = sns.boxplot(x=obs_data.Elev_bin, y=obs_data.Ba, hue=obs_data.Year, palette='BuGn', saturation=0.7, ax=ax)\n",
    "    # # ax = sns.boxplot(x=Z_F, y=OB_F, palette='BuGn', saturation=0.7)#, ax=ax)\n",
    "    # meanlineprops = dict(linestyle='--', linewidth=1., color='0.5')\n",
    "    # medianprops = dict(linestyle='-', linewidth=1, color='k')\n",
    "    # BOX=plt.boxplot(OB_F[1:],meanprops=meanlineprops,medianprops=medianprops,showmeans=True, meanline=True,sym='',\n",
    "    #                 positions=[1950, 2050, 2150, 2250, 2350, 2450, 2550, 2650, 2750, 2850, 2950, 3050,\n",
    "    #                           3150, 3250],widths=75)\n",
    "    # #                            2050., 2150., 2250., 2350., 2450., 2550., 2650., 2750.\n",
    "    # # Add jitter with the swarmplot function.\n",
    "    # # ax = sns.swarmplot(x=Zor, y=OBS, color=\"grey\")\n",
    "    # # glacio = [Z, OBS]\n",
    "\n",
    "    # # ax.boxplot(glacio)\n",
    "\n",
    "    # ############################\n",
    "    # ax.set_xlim(1880,3300)\n",
    "    # fig.subplots_adjust(bottom=0.12, top=0.98, hspace=0.1, left=0.08, right=0.99, wspace=0.05)#left=0.07, right=0.9,wspace=0.05, \n",
    "    # ax.legend(loc='upper left')#, bbox_to_anchor=(0, 0.42, 0.7, 0.5), labelspacing=0.2, handletextpad=0.1)\n",
    "    # # ax.xaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "    # fig.text(0.01, 0.75, 'Mass balance (m w.e.)', rotation=90)\n",
    "    # fig.text(0.45, 0.01, 'Elevation (m a.s.l.)')\n",
    "    # fig.text(0.72, 0.35, str(year)+ ' '+ balance, fontweight='bold')\n",
    "    # plt.savefig(fl_path + 'products/' + Glacier[gl]+'_bdot_split' + balance + str(year) +'.png', dpi=300) #+ Glacier[gl]\n",
    "\n",
    "    ######################## end split grad fig ##########################\n",
    "    \n",
    "    font = {'family' : 'Arial', 'weight' : 'normal', 'size'   : 8}\n",
    "    plt.rc('font', **font)\n",
    "    pylab.rcParams['xtick.major.pad']='1'\n",
    "    pylab.rcParams['ytick.major.pad']='1'\n",
    "    \n",
    "    \n",
    "    n = 0\n",
    "    s= 10 #markersize\n",
    "\n",
    "    a = 0.7\n",
    "    color=['k', 'lime', 'green', 'teal']\n",
    "\n",
    "    obs=obs[(obs.Year==year)]\n",
    "    obs.reset_index(inplace=True)\n",
    "\n",
    "    T =pd.DataFrame()\n",
    "    for i in range(len(obs.name)):\n",
    "        if 'W' not in obs.name[i] and 'w' not in obs.name[i] :\n",
    "            T=T.append(obs[obs.index == i],ignore_index=True)\n",
    "\n",
    "    obs = T\n",
    "\n",
    "    ##all_obs\n",
    "    if balance == 'Bw':\n",
    "        obs = obs.dropna(subset=['Bw'])\n",
    "        y_ax_obs=obs[(obs.Year==year)].Bw\n",
    "\n",
    "    else:\n",
    "        obs = obs.dropna(subset=['Ba'])\n",
    "        y_ax_obs=obs[(obs.Year==year)].Ba\n",
    "\n",
    "    x_ax_obs=obs[(obs.Year==year)].Elev\n",
    "\n",
    "    ##obs by bin\n",
    "    # x_ax_obs=FG_df._elevmean  #FG_df.elev_mean\n",
    "    # y_ax_obs=FG_df.b_obs16#b_obs\n",
    "\n",
    "    # Drop unrealistic FG smb:\n",
    "    new = FG_all.copy()\n",
    "\n",
    "    # new = new[new.b_fg_we >-8.]\n",
    "    # new = new[new.b_fg_we_gpr >-8.]\n",
    "\n",
    "    new_gpr = new.copy()\n",
    "    new_gpr = new_gpr.drop([13]) #new_gpr.dropna(subset=['b_fg_we_gpr'])\n",
    "\n",
    "    #errors (above and below)\n",
    "    # new['yerr'] = 0.20\n",
    "    yerr = [0.0, new_gpr.sBwe_gpr, new.sBwe_opt, new.sBwe_F] #new.sBwe_gpr\n",
    "    new['xerr'] = new.dem_std * 1.5 #2 std dev -- 95% of data\n",
    "    xerr = new.xerr \n",
    "\n",
    "    ### ALL points ###\n",
    "    x_ax_fg_gpr=new_gpr.dem_mean\n",
    "    y_ax_fg_gpr=new_gpr.b_fg_we_gpr\n",
    "\n",
    "    x_ax_fg_opt=new.dem_mean  #FG_df.elev_mean\n",
    "    y_ax_fg_opt=new.b_fg_we\n",
    "\n",
    "    x_ax_fg_F=new.dem_mean  \n",
    "    y_ax_fg_F=new.b_fg_weF\n",
    "\n",
    "    x = [x_ax_obs, x_ax_fg_gpr, x_ax_fg_opt, x_ax_fg_F]\n",
    "    y = [y_ax_obs, y_ax_fg_gpr, y_ax_fg_opt, y_ax_fg_F]\n",
    "    \n",
    "    letter='a','b','c','d'\n",
    "    color = ['k', 'teal', '#74c476', '#238b45']\n",
    "    label = ['Obs.', 'Fg gpr', 'Fg opt.','Farinotti']\n",
    "    sym = ['o', '^', 's', 'd']\n",
    "    # Call function to create error bars \n",
    "    shift = [-45,0,45]\n",
    "    ax1[count].scatter(x[0],y[0],color=color[0], label=label[0], alpha=a+.2, s=s,facecolor='', zorder=3)\n",
    "    for i in range(3):\n",
    "        ax1[count].errorbar((x[i+1]+shift[i]), y[i+1], xerr=None, yerr=yerr[i+1], fmt=sym[i+1], ecolor=color[i+1], zorder=2,\n",
    "                              label=label[i+1], alpha=0.4, c=color[i+1]) #elinewidth=0.7\n",
    "    #     _ = make_error_boxes(ax, x[i+1], y[i+1], xerr, yerr, ecolor=color[i+1])  #[0]\n",
    "    #     _ = make_error_boxes(ax, x[i+1], y[i+1], xerr, yerr, hue=color[i+1])[1]\n",
    "\n",
    "    ## create range bar to represent flux gate elevation range\n",
    "    # theives = ax.errorbar(x[2], y[2], xerr=xerr[2], yerr=None, fmt='None', ecolor=color[2], zorder=1,\n",
    "    #                           elinewidth=8, marker='d', label='Fg range', ls='..', alpha=0.25)\n",
    "\n",
    "    ## plot data and regression lines\n",
    "    ytxt = [0.2, 0.15, 0.1, 0.05]\n",
    "    for i in range(4):\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(x[i]),np.array(y[i]))\n",
    "    #     ax.scatter(x[i],y[i],color=color[i], label=label[i], alpha=a, s=s,facecolor='', zorder=3)\n",
    "\n",
    "    #     ax = sns.boxplot(x=x[i], y=y[i], palette='copper', saturation=0.7)#, ax=ax)\n",
    "    #     ax.plot(x[i],slope*x[i]+intercept, color=color[i],label='Lin', alpha=a, linewidth=0.9, zorder=3)\n",
    "    #     print(slope, intercept, r_value, p_value, std_err)\n",
    "        txt= ['Obs. slope:        ', 'FG gpr slope:    ', 'FG opt. slope:   ', 'FG Farin. slope: ']\n",
    "\n",
    "    #     'H_{opt}''slope:$'\n",
    "\n",
    "        #################### RLM  https://www.statsmodels.org/stable/rlm.html\n",
    "        ###https://www.statsmodels.org/devel/generated/statsmodels.robust.robust_linear_model.RLMResults.html\n",
    "        exog = stm.add_constant(x[i]) #obs_data.Elev)\n",
    "        rlm_model = stm.RLM(y[i],exog, M=stm.robust.norms.HuberT()) #data.endog, data.exog,\n",
    "        rlm_results = rlm_model.fit()\n",
    "        stdE=rlm_results.bse[1]\n",
    "        # print(rlm_results.params)\n",
    "         # ax.scatter(x[i],y[i],color=color[i], label='data', alpha=a, s=s,facecolor='', zorder=3)\n",
    "        #     ax = sns.boxplot(x=x[i], y=y[i], palette='copper', saturation=0.7)#, ax=ax)\n",
    "        ax1[count].plot(x[i],rlm_results.params[1]*x[i]+rlm_results.params[0], color=color[i],label='', alpha=a, \n",
    "                linewidth=0.8, zorder=3) # linestyle='--'\n",
    "        print('slope RLM:', rlm_results.params[1])\n",
    "        #############################\n",
    "        ax1[count].text(0.2, ytxt[i], txt[i]+str(np.round(1000*rlm_results.params[1],2))+ ' +/- ' + \n",
    "                str(np.round(1000*stdE,2)), transform=ax1[count].transAxes)\n",
    "\n",
    "    if balance == 'Ba':\n",
    "        ax1[count].axhline(linewidth=1, color='k', ls='--', alpha=a+.2, zorder=0)\n",
    "    # ax.axvline(2475.,linewidth=1, color='k', ls='--', alpha=a-0.2, zorder=0)\n",
    "\n",
    "\n",
    "    ############ obs data box plot ################\n",
    "    obs_data=obs[(obs.Year==year)]\n",
    "    obs_data.reset_index(inplace=True)\n",
    "    bin_size = 100.\n",
    "    z_range = np.arange(1850., 3350., bin_size)\n",
    "    # print(z_range)\n",
    "    # obs_data['Elev_bin'] = np.nan\n",
    "\n",
    "    i = 0\n",
    "    # for n in range(len(obs_data.Ba)):\n",
    "\n",
    "    Z_F = []\n",
    "    Zor_F= []\n",
    "    OB_F=[]\n",
    "\n",
    "    for z in z_range: \n",
    "        OBS = []\n",
    "        Z = []\n",
    "        Zor = []\n",
    "        W=0\n",
    "        bin_size = 100.\n",
    "        while W==0:\n",
    "            for n in range(len(obs_data[balance])):\n",
    "                if ((z - bin_size/2.) <= obs_data.Elev[n]) and (obs_data.Elev[n] <=  (z + bin_size/2.)):\n",
    "                    O = obs_data[balance][n]\n",
    "                    Z.append(z)\n",
    "                    Zor.append(obs_data.Elev[n])\n",
    "                    OBS.append(O)\n",
    "\n",
    "            if len(OBS)<2:\n",
    "                ##could select last element below elevation\n",
    "                bin_size=bin_size*1.5\n",
    "            else:\n",
    "                OB_F.append(np.array(OBS))\n",
    "                Z_F.append(np.array(Z))\n",
    "                Zor_F.append(np.array(Zor))\n",
    "\n",
    "                W=1\n",
    "        i += 1\n",
    "\n",
    "    meanlineprops = dict(linestyle='--', linewidth=1., color='0.5')\n",
    "    medianprops = dict(linestyle='-', linewidth=1, color='k')\n",
    "    BOX=ax1[count].boxplot(OB_F[1:],meanprops=meanlineprops,medianprops=medianprops,showmeans=True, meanline=True,sym='',\n",
    "                    positions=[1950, 2050, 2150, 2250, 2350, 2450, 2550, 2650, 2750, 2850, 2950, 3050,\n",
    "                              3150, 3250],widths=75)\n",
    "\n",
    "##################Difference###########################################################\n",
    "       \n",
    "    GPR = []\n",
    "    OPT = []\n",
    "    FAR = []  \n",
    "    for z in z_range:\n",
    "        GG=[]\n",
    "        PP=[]\n",
    "        FF=[]\n",
    "        for n in range(len(new.dem_mean)):\n",
    "            if ((z - bin_size/2.) <= new.dem_mean[n]) and (new.dem_mean[n] <=  (z + bin_size/2.)):\n",
    "                G = new.b_fg_we_gpr[n]\n",
    "                P = new.b_fg_we[n]\n",
    "                F = new.b_fg_weF[n]\n",
    "                GG.append(G)\n",
    "                FF.append(F)\n",
    "                PP.append(P)\n",
    "                \n",
    "        GPR.append(np.array(GG))\n",
    "        OPT.append(np.array(PP))\n",
    "        FAR.append(np.array(FF))\n",
    "        \n",
    "\n",
    "    if count == 0:\n",
    "        gpr_bdiff=[]\n",
    "        opt_bdiff=[]\n",
    "        farin_bdiff=[]\n",
    "       \n",
    "    OBF=[]\n",
    "    GPRq=[]\n",
    "    OPTq=[]\n",
    "    FARq=[]\n",
    "    for i in range(len(OB_F[1:])):\n",
    "        OBF.append(np.round(OB_F[i].mean(),3))\n",
    "        GPRq.append(np.round(GPR[i].mean(),3))\n",
    "        OPTq.append(np.round(OPT[i].mean(),3))\n",
    "        FARq.append(np.round(FAR[i].mean(),3))\n",
    "        \n",
    "    gpr_bdiff.extend((np.array(OBF) - np.array(GPRq)))\n",
    "    opt_bdiff.extend((np.array(OBF) - np.array(OPTq)))\n",
    "    farin_bdiff.extend((np.array(OBF) - np.array(FARq)))\n",
    "    \n",
    "###################End difference##########################\n",
    "\n",
    "    ax1[count].set(xlim=(1850,3300), ylim=(-9.5,3.2))\n",
    "    fig1.subplots_adjust(bottom=0.10, top=0.98, hspace=0.1, left=0.06, right=0.99, wspace=0.05)#left=0.07, right=0.9,wspace=0.05, \n",
    "    ax1[0].legend(loc='upper left')#, bbox_to_anchor=(0, 0.42, 0.7, 0.5), labelspacing=0.2, handletextpad=0.1)\n",
    "    ax1[count].xaxis.set_major_locator(ticker.MultipleLocator(200))\n",
    "    fig1.text(0.01, 0.75, 'Mass balance (m w.e.)', rotation=90)\n",
    "    fig1.text(0.45, 0.01, 'Elevation (m a.s.l.)')\n",
    "    ax1[count].text(0.95, 0.05, letter[count], fontweight='bold', transform=ax1[count].transAxes)\n",
    "    ax1[count].set_xticklabels(['',2000, '', 2400, '', 2800, '',3000, ''])\n",
    "    ################### mass conservation plot ####################################\n",
    "    BM= []\n",
    "    for i in range(len(OB_F[1:])):\n",
    "        B = OB_F[1:][i].mean()\n",
    "        BM.append(B)\n",
    "    BM = np.squeeze(BM)\n",
    "    BM = {'SMB': BM}\n",
    "    BM = pd.DataFrame(data=BM, index=None)\n",
    "    new['SMB'] = BM\n",
    "    new['BMC'] = (new.SMB * new['rho_%s'%year]/1000.) + new.Q_netA + new.Vfirn \n",
    "    new['Bdh'] = new.dh_mean   \n",
    "\n",
    "    new_gpr['BMC_gpr'] = (new.SMB[:-1] * new_gpr['rho_%s'%year]/1000.) + new_gpr.Q_netA_gpr + new_gpr.Vfirn \n",
    "    new['BMCF'] = (new.SMB * new['rho_%s'%year]/1000.) + new.Q_netAF + new.Vfirn \n",
    "    \n",
    "#     new.loc[13, 'BMC_gpr'] = new.BMC[13]\n",
    "    # shiftMC = [10, 20, 30]\n",
    "    MC = [new_gpr.BMC_gpr,new.BMC, new.BMCF]\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            ax2[count].errorbar(new_gpr.dem_mean+shift[i]+5,MC[i], xerr=None, yerr=yerr[i+1], fmt=sym[i+1], \n",
    "                     c=color[i+1], label=label[i+1], alpha=0.7, zorder=2)\n",
    "        else:\n",
    "            ax2[count].errorbar(new.dem_mean+shift[i],MC[i], xerr=None, yerr=yerr[i+1], fmt=sym[i+1], \n",
    "                     c=color[i+1], label=label[i+1], alpha=0.7, zorder=2)\n",
    "    \n",
    "    ax2[count].errorbar(new.dem_mean,new.Bdh, xerr=50, yerr=0.50, fmt='o', c='k', label='LiDAR', \n",
    "                alpha=0.8, zorder=3, mfc='none')\n",
    "    ax2[count].text(0.95, 0.95, letter[count], fontweight='bold', transform=ax2[count].transAxes)\n",
    "    ax2[count].set_xticklabels(['',2000, '', 2400, '', 2800, '',3000, ''])\n",
    "    ax1[count].set(ylim=(-9.,3.5)) #xlim=(1850,3300)\n",
    "    ax2[0].legend(loc='best')\n",
    "    ax2[1].set_xlabel('Elevation (m a.s.l.)')\n",
    "    ax2[0].set_ylabel('Height change (m ice $a^{-1}$)')\n",
    "    fig2.subplots_adjust(bottom=0.125, top=0.99, hspace=0.1, left=0.10, right=0.99, wspace=0.05)\n",
    "################# end mass conservation plot ####################################\n",
    "    count+=1\n",
    "    \n",
    "#     print(FG_all.spQo_gpr.median())\n",
    "#     print(FG_all.spQout.median())\n",
    "#     print(FG_all.spQoutF.median())\n",
    "    print(FG_all.sBwe_gpr.mean())\n",
    "    print(FG_all.sBwe_opt.mean())\n",
    "    print(FG_all.sBwe_F.mean())\n",
    "fig1.savefig(fl_path + 'products/' + Glacier[gl]+'_bdot_dint0.8' + balance + '_all_years.png', dpi=300) #+ Glacier[gl]\n",
    "fig2.savefig(fl_path + 'products/' + glacier[gl]+'_mass_con_dint0.8' + balance +'_all_years.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # season\n",
    "# balance = 'Ba'  # Bw or Ba (winter or annual)\n",
    "# # rho_snow = 445. # 470 kg m-3 geodetic or 457 glaciological, or...take given years obs. value\n",
    "# year = 2017 # any of: 2016, 2017, 2018\n",
    "# I = np.abs(year - 2016) # for selecting raster inputs\n",
    "# vcorr=False ## is velocity corrected by off ice bias and stake bias?\n",
    "\n",
    "# gl = 1 # number to select glacier\n",
    "# Glacier = ['Kokanee', 'Conrad', 'Illecillewaet']\n",
    "# glacier = ['kokanee', 'conrad', 'illecillewaet']\n",
    "# d = 20. #20.  #distance (m) between flux gate points\n",
    "# Dint = 0.8 #depth integrated velocity ratio, 1.0= all sliding\n",
    "# sVZ = 4.91  #velocity against stake measurements\n",
    "# sHopt = 0.1 #10.3 ## ME:10.3 m,  MAE:0.223 (percent ratio)\n",
    "# cb = 7 ##center bin use this to assure flux in is from bins cb+1 and cb+2\n",
    "\n",
    "# fl_path = '/home/pelto/Desktop/ice_flux/' + Glacier[gl] + '/'\n",
    "# gpr_path = '/home/pelto/GIS/GPR/ComparisonData/'\n",
    "# path = '/home/pelto/Desktop/lidar_cbt_analysis/' + glacier[gl] + '/'\n",
    "\n",
    "# VDIR = 'average'  # 'average' or 'individual'\n",
    "\n",
    "# if VDIR == 'individual':  \n",
    "       \n",
    "#     ## DEMs\n",
    "#     vf_list = ['img1_20160417__img1_20170521_vmap_10m_35px_spm2/', 'img1_20160417__img1_20170521_vmap_10m_35px_spm2/',\n",
    "#            'img1_20170904__img2_20180822_vmap_10m_35px_spm2/'] #'img1_20170521__img2_20180426_vmap_10m_35px_spm2/']##backups  img1_20170917__img1_20180426_vmap_5m_35px_spm2/\n",
    "\n",
    "#     ##planet labs\n",
    "# #     vf_list = ['xx', 'xx',\n",
    "# #            'img1_20170904__img2_20180822_vmap_10m_35px_spm2/']\n",
    "    \n",
    "#     vdir = '/home/pelto/Desktop/velocity_mapping/' +Glacier[gl] + '_DEMs/bedem5_spm2/' + vf_list[I]\n",
    "# #     vdir = '/home/pelto/Desktop/velocity_mapping/conrad_planet/' + vf_list[I]\n",
    "#     VX = vdir+ vf_list[I][:-1] + '-F_vx.tif' \n",
    "#     VY = vdir+ vf_list[I][:-1] + '-F_vy.tif' \n",
    "\n",
    "# # # if VDIR == 'individual':  \n",
    "# #     vf_list = ['img1_20160417__img1_20170521_vmap_5.0m_35px_spm2/', 'img1_20160417__img1_20170521_vmap_5.0m_35px_spm2/',\n",
    "# #            'img1_20170521__img2_20180426_vmap_5.0m_35px_spm2/']##backups  img1_20170917__img1_20180426_vmap_5m_35px_spm2/\n",
    "# #     vdir = '/home/pelto/Desktop/velocity_mapping/' +Glacier[gl] + '_DEMs/spm2/' + vf_list[I]\n",
    "# #     VX = vdir+ vf_list[I][:-1] + '-F_vx.tif' \n",
    "# #     VY = vdir+ vf_list[I][:-1] + '-F_vy.tif' \n",
    "# else:\n",
    "# #     vf_list = ['illec_all_spm2_5m', 'illec_all_spm2_5m', 'illec_all_spm2_5m']\n",
    "#     vdir = '/home/pelto/Desktop/velocity_mapping/Conrad_DEMs/spm2/3m/'#bedem5_spm2/'  \n",
    "#     VX = vdir + 'conrad_all_vx_3mdems_5m.tif' #'conrad_vx_spring_10m.tif' \n",
    "#     VY = vdir + 'conrad_all_vy_3mdems_5m.tif' #'conrad_vy_spring_10m.tif'\n",
    "#     VM = vdir + 'conrad_all_vm_3mdems_5m.tif'\n",
    "# ## velocity_mapping/conrad_planet/img1_20170904__img2_20180822_vmap_10m_35px_spm2\n",
    "\n",
    "# topo = '/home/pelto/Desktop/lidar_cbt_analysis/conrad/20160912_conrad_dem1_clip_slave.tif'\n",
    "# topo_big = '/home/pelto/Desktop/lidar_cbt_analysis/conrad/dem_mosaic_conrad.tif'\n",
    "# farinotti = gpr_path + 'RGI60-02.02171_thickness.tif'\n",
    "\n",
    "# dhW_list = ['conrad_2016_winter_dh_dt14s.tif', 'conrad_2017_winter_dh_dt16s.tif', 'conrad_2018_winter_dh_dt.tif']\n",
    "# dhA_list = ['conrad_2015_2016_dh_dt_filled_1416+50cm.tif', 'conrad_2016_2017_dh_dt_filled.tif','conrad_2018_2017_dh_dt_60.tif']\n",
    "# if balance == 'Bw':\n",
    "#     dh_r = path+ dhW_list[I]   #winter height change TIFF\n",
    "# else:\n",
    "#     dh_r = path+  dhA_list[I] #Annual height change TIFF\n",
    "\n",
    "# pts_file = fl_path + 'conrad_points_17gates_C_20m_wgs84_b.shp' #'conrad_points_17gates_20m_wgs84.shp'  ##must be WGS84 currently\n",
    "# gates = fl_path+'conrad_flux_gates_17_C.shp'\n",
    "# shpf = path + Glacier[gl] + '/conrad_all_glaciers_2014.shp'  #GLIMS_BC/glims_all/all_glaciers_2016.shp' \n",
    "# bins=fl_path+glacier[gl]+'_bins_2017_C'+'.shp' #+str(year)\n",
    "\n",
    "# obs = pd.read_csv(fl_path+ 'Conrad_bdot.csv') \n",
    "# rho = pd.read_csv(fl_path + 'conrad_rho.csv')  ## rho\n",
    "\n",
    "# # open GeoTIFF as array\n",
    "\n",
    "# vx = salem.open_xr_dataset(VX)  #gdal.Open(vx).ReadAsArray()\n",
    "# vy = salem.open_xr_dataset(VY)#gdal.Open(vy).ReadAsArray()\n",
    "# vm = salem.open_xr_dataset(VM)\n",
    "# msk_office = salem.open_xr_dataset('/home/pelto/Desktop/lidar_cbt_analysis/conrad/conrad_total_msk.tif') \n",
    "# msk = salem.open_xr_dataset(fl_path + 'conrad_2016_ice_msk_5m_glacieronly.tif') \n",
    "# msk_conrad = salem.open_xr_dataset('/home/pelto/Desktop/lidar_cbt_analysis/conrad/conrad_2014_extent_5m.tif')\n",
    "\n",
    "# gpr = salem.open_xr_dataset(fl_path + 'gpr_outline_25_100m_5m.tif') #'gpr_outlines_all_25_25m_re5m.tif'\n",
    "# farin = salem.open_xr_dataset(farinotti)\n",
    "\n",
    "# H_opt = salem.open_xr_dataset(fl_path+'opt_thick_101sw_MAE_5m.tif') \n",
    "# #     pts = salem.read_shapefile(fl_path+pts)\n",
    "# gates = salem.read_shapefile(gates)\n",
    "\n",
    "# vx = vx.to_array(name='vx')\n",
    "# # np.shape(vx[0]) remove index 1 i.e.: 1,2000,3000\n",
    "# vy = vy.to_array(name='vy')\n",
    "# vm = vm.to_array(name='vm')\n",
    "\n",
    "# gpr_reproj = vx.salem.transform(gpr)\n",
    "# gpr = gpr_reproj.to_array(name='gpr')\n",
    "\n",
    "# msk_reproj = vx.salem.transform(msk)  #note succeeding trying to use gdalwarp to go from 2955 --> 32611\n",
    "# msk = msk_reproj.to_array(name='msk')\n",
    "# msk_office_reproj = vx.salem.transform(msk_office)  #note succeeding trying to use gdalwarp to go from 2955 --> 32611\n",
    "# msk_off_ice = msk_office_reproj.to_array(name='msk')\n",
    "# msk_conrad_reproj = msk_conrad.salem.transform(msk_conrad)\n",
    "# msk_conrad = msk_conrad_reproj.to_array(name='msk_conrad')\n",
    "\n",
    "# # farin = farin.to_array(name='vx')\n",
    "# H_opt_reproj = vx.salem.transform(H_opt)\n",
    "# H_opt = H_opt_reproj.to_array(name='H_opt')\n",
    "\n",
    "# farin_reproj= vx.salem.transform(farin)\n",
    "# farin = farin_reproj.to_array(name='farin')\n",
    "# srtm_corr = fl_path + 'conrad_SRTM_diff_30m.tif'\n",
    "# srtm_corr = salem.open_xr_dataset(srtm_corr)\n",
    "# srtm_corr = vx.salem.transform(srtm_corr)\n",
    "# srtm_corr = srtm_corr.to_array(name='srtm_corr')\n",
    "# srtm_corr.data[srtm_corr.data>10.0] = 0.0  ##remove positive anomalous values\n",
    "# srtm_corr.data[srtm_corr.data<-50.0] = 0.0 ##remove negative anomalous values\n",
    "# farin_corr = farin + srtm_corr\n",
    "# farin_corr= farin_corr.rename('farin_corr')\n",
    "\n",
    "# gpr.data[gpr.data<0] = np.nan\n",
    "# gpr.data[gpr.data<1.5] = 0.0   #not having any effect\n",
    "\n",
    "# vx.data[msk.data==0] = np.nan\n",
    "# vy.data[msk.data==0] = np.nan\n",
    "# vm.data[msk.data==0] = np.nan\n",
    "# VZ = np.sqrt(vx.data**2 + vy.data**2)\n",
    "\n",
    "# VZ_off_ice = VZ.copy()\n",
    "# VZ_off_ice[msk_off_ice.data>0.0] = np.nan\n",
    "\n",
    "# dem = salem.open_xr_dataset(topo)\n",
    "# dem_reproj = vx.salem.transform(dem)\n",
    "# dem = dem_reproj.to_array(name='dem')\n",
    "# dem.data[dem.data<1] = np.nan\n",
    "\n",
    "# #     # map extent\n",
    "# grid = mercator_grid(center_ll=(-117.00, 50.78), extent=(5000, 5000)) ##zoomed out view\n",
    "# # 487892.000 5509738.000 491232.000 5512358.000\n",
    "# grid = vx.salem.grid  ##full view\n",
    "# sm = Map(grid, countries=False)\n",
    "# sm.set_lonlat_contours(interval=0)\n",
    "# sm.set_scale_bar()\n",
    "# sm.set_data(vx) \n",
    "# # sm.set_vmax(val=1.)\n",
    "# #      Change the lon-lat countour setting\n",
    "# sm.set_lonlat_contours(add_ytick_labels=True, interval=0.05, linewidths=0.75, linestyles='--', colors='0.25')\n",
    "# off_ice_V = np.nanmean(VZ_off_ice)\n",
    "# print(off_ice_V)\n",
    "# print(np.nanstd(VZ_off_ice))\n",
    "\n",
    "# gdf = salem.read_shapefile(shpf)\n",
    "# sm.set_shapefile(gdf, linewidth=1)\n",
    "# sm.set_shapefile(gates, linewidth=1.5, color='r')\n",
    "# sm.visualize()\n",
    "# # plt.append_colorbar(ax, label='Velocity (m yr-1)')\n",
    "# plt.savefig(fl_path+ 'products/'+ glacier[gl]+ '_' +str(year) +'_thickness_gates.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # D[D.ID==0].Qopt = D.vfg * 1.0 * 20. * D.H_opt\n",
    "    # D[D.ID==1].Qopt = D.vfg * 1.0 * 20. * D.H_opt\n",
    "\n",
    "#     KP=[]\n",
    "#     KF=[]\n",
    "#     # Ice flux where lowest two bins have sliding velocity equal to surface velocity\n",
    "#     for loop in D.index:\n",
    "#         ID= D[D.index==loop].ID.values\n",
    "#     #     if ((ID==0) | (ID==1)):     \n",
    "#     #         KP.append( ((D[D.index==loop].vfg - off_ice_V)* (Dint+0.1) * d * D.H_opt[D.index==loop]).values[0])  \n",
    "#     #         KF.append( ((D[D.index==loop].vfg - off_ice_V)* (Dint+0.1) * d * D.farin_corr[D.index==loop]).values[0])\n",
    "#     #     else:\n",
    "#         KP.append((D[D.index==loop].vfg * Dint * d * D.H_opt[D.index==loop]).values[0])  ## ice flux per slice *0.9 to est. depth-integrated velocity\n",
    "#         KF.append((D[D.index==loop].vfg * Dint * d * D.farin_corr[D.index==loop]).values[0])\n",
    "#     D['Qopt'] = KP\n",
    "#     D['Qfarin'] = KF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    topo_list = ['150911_illecillewaet_dem1_dhdt16.tif', '160912_illecillewaet_dem1_clip_master.tif', '170917_illecillewaet_dem1_clip_slave.tif']\n",
    "    topo = path + topo_list[I]\n",
    "    dhdt16 = salem.open_xr_dataset(path + dhA_list[0])\n",
    "    dhdt17 = salem.open_xr_dataset(path + dhA_list[1])\n",
    "    dhdt16_reproj = vx.salem.transform(dhdt16)\n",
    "    dhdt16 = dhdt16_reproj.to_array(name='dhdt16')\n",
    "    dhdt17_reproj = vx.salem.transform(dhdt17)\n",
    "    dhdt17 = dhdt17_reproj.to_array(name='dhdt17')\n",
    "        ##acount for height change on measured ice thickness\n",
    "    if year == 2016:\n",
    "        dh16 = D.dhdt16 - D.dhdt17\n",
    "        D.H_opt = D.H_opt - dh16\n",
    "        D.farin_corr = D.farin_corr - dh16\n",
    "        D.gpr = D.gpr - dh16\n",
    "    elif year == 2017:\n",
    "        D.H_opt = D.H_opt - D.dhdt17\n",
    "        D.farin_corr = D.farin_corr - D.dhdt17\n",
    "        D.gpr = D.gpr - D.dhdt17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=obs.Ba.values\n",
    "X=obs.Elev.values\n",
    "X = stm.add_constant(X)\n",
    "model=stm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "print('Parameters: ', results.params)\n",
    "print('Slope: ', results.params[1]*1000)\n",
    "print('SE: ',results.bse[1]*1000)\n",
    "print('R2: ', results.rsquared)\n",
    "\n",
    "print(results.rsquared)\n",
    "results.bse\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(obs_data.Ba)   #new.b_fg_we.values\n",
    "x = np.array(obs_data.Elev) #new.dem_mean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_linear(x, x0, y0, k1, k2):\n",
    "    x0=2600\n",
    "#     print(k1,k2)\n",
    "    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])\n",
    "# print(piecewise_linear)\n",
    "plt.figure(figsize=(10,8))\n",
    "p , e = curve_fit(piecewise_linear, x, y)\n",
    "xd = np.linspace(2000, 3100, 15)\n",
    "plt.plot(x, y, \"o\") #, mfc=None)\n",
    "plt.plot(xd, piecewise_linear(xd, *p), color='k')\n",
    "plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean square error of the lines\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "x0=2600\n",
    "\n",
    "MSE_results=[]\n",
    "for Line in range(2):\n",
    "    Pred=[];Act=[]\n",
    "    if Line ==0: ##mean error line 1 (ablation zone gradient)\n",
    "        INDEX= np.where(x<x0)[0]\n",
    "        for ix in INDEX:\n",
    "            Pred.append( p[2]*x[ix]+(p[1]-p[2]*x0))\n",
    "            Act.append(y[ix])\n",
    "            E0=np.array(Pred)-np.array(Act) #error\n",
    "            SE0=np.std(E)/np.sqrt(len(Act))\n",
    "            \n",
    "        MSE_results.append(MSE(Act,Pred)) \n",
    "    if Line==1:  ##mean error line 2 (accumulation zone gradient)\n",
    "        INDEX= np.where(x>=x0)[0]\n",
    "        for ix in INDEX:\n",
    "            Pred.append( p[3]*x[ix]+(p[1]-p[3]*x0))\n",
    "            Act.append(y[ix])\n",
    "            E1=np.array(Pred)-np.array(Act) #error\n",
    "            SE1=np.std(E)/np.sqrt(len(Act))\n",
    "            \n",
    "        MSE_results.append(MSE(Act,Pred)) \n",
    "MSE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.round(p[2]*1000,2),u\"\\u00B1\" ,np.round(MSE_results[0]*1000,2), np.round(p[3]*1000,2), u\"\\u00B1\", np.round(MSE_results[1]*1000,2))\n",
    "print(np.round(p[2]*1000,2),u\"\\u00B1\", np.round(SE0*1000,2), np.round(p[3]*1000,2), u\"\\u00B1\", np.round(SE1*1000,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=np.array(Pred)-np.array(Act) #error\n",
    "SE=np.std(E)/np.sqrt(len(Act))\n",
    "SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = pd.read_csv(fl_path+ 'Conrad_bdot.csv') \n",
    "# obs.dropna(subset=['Ba'],inplace=True)\n",
    "\n",
    "# Y=np.array(obs.Ba)\n",
    "# X=np.array(obs.Elev)\n",
    "# model = pwlf.PiecewiseLinFit(X, Y)\n",
    "# breakpoints = [1950.,2500.,3300.]\n",
    "# model.fit_with_breaks(breakpoints)\n",
    "\n",
    "# x_hat = np.linspace(X.min(), X.max(), 100)\n",
    "# y_hat = model.predict(x_hat)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X, Y, 'o')\n",
    "# plt.plot(x_hat, y_hat, '-')\n",
    "# plt.grid()\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.show()\n",
    "\n",
    "# SL=model.slopes\n",
    "# SE=model.standard_errors()\n",
    "# SE\n",
    "\n",
    "# print(round(SL[0]*1000,2), round(SE[1]*1000*1.96,2))\n",
    "# print(round(SL[1]*1000,2), round(SE[2]*1000*1.96,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = np.concatenate((model.beta,\n",
    "#                              model.fit_breaks[1:-1]))\n",
    "\n",
    "# p = model.p_values(method='non-linear', step_size=1e-4)\n",
    "# se = model.se  # standard errors\n",
    "\n",
    "# header = ['Parameter type', 'Parameter value', 'Standard error', 't',\n",
    "#           'P > np.abs(t) (p-value)']\n",
    "\n",
    "# print(*header, sep=' | ')\n",
    "\n",
    "\n",
    "# values = np.zeros((parameters.size, 5), dtype=np.object_)\n",
    "# values[:, 1] = np.around(parameters, decimals=5)\n",
    "# values[:, 2] = np.around(se, decimals=5)\n",
    "# values[:, 3] = np.around(parameters / se, decimals=5)\n",
    "# values[:, 4] = np.around(p, decimals=5)\n",
    "\n",
    "# for i, row in enumerate(values):\n",
    "#     if i < model.beta.size:\n",
    "#         row[0] = 'Beta'\n",
    "#         print(*row, sep=' | ')\n",
    "#     else:\n",
    "#         row[0] = 'Breakpoint'\n",
    "#         print(*row, sep=' | ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zr_gpr=[]\n",
    "# for i in range(3):\n",
    "#     zr_gpr.extend(z_range)\n",
    "# zr_gpr\n",
    "# gpr_ALL=np.array(gpr_all)\n",
    "# gpr_ALL = gpr_ALL[np.logical_not(np.isnan(gpr_ALL))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'Helvetica', 'weight' : 'normal',  'size'   : 8}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "obs = pd.read_csv(fl_path+ 'Conrad_bdot.csv') \n",
    "obs.dropna(subset=['Ba'],inplace=True)\n",
    "# obs=obs[obs.Year==2016 and obs.Year==2017 and obs.Year==2018]\n",
    "\n",
    "############## piecewise function ########################################         \n",
    "fig3, ax3 = plt.subplots(1, sharex=True, sharey=True, figsize=(8,8))#(3.5,3.5)) \n",
    "y_bin=[obs.Ba,gpr_all,opt_all,farin_all]; x_bin=[obs.Elev,elev_gpr_all,elev_all,elev_all]\n",
    "# y_bin=[obs_all,gpr_ALL,opt_all,farin_all]; x_bin=[z_range_all,zr_gpr,z_range_all,z_range_all]\n",
    "def piecewise_linear(x, x0, y0, k1, k2):\n",
    "    x0=2500\n",
    "    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])   \n",
    "label=['Observed', 'FG IPR', 'FG OGGM', 'FG Farinotti']\n",
    "for i in range(4):      \n",
    "#         plt.figure(figsize=(10,8))\n",
    "    x1=np.array(x_bin[i]); y1=np.array(y_bin[i])\n",
    "    p , e = curve_fit(piecewise_linear, x1, y1 )\n",
    "\n",
    "    xd = np.linspace(1900, 3200, 15)\n",
    "    ax3.plot(xd, piecewise_linear(xd, *p), color=color[i], label=label[i])\n",
    "    # mean square error of the lines\n",
    "    x0=2450\n",
    "\n",
    "    MSE_results=[]\n",
    "    for Line in range(2):\n",
    "        Pred=[];Act=[]\n",
    "        if Line ==0:\n",
    "            INDEX= np.where(x1<x0)[0]\n",
    "            for ix in INDEX:\n",
    "                Pred.append( p[2]*x1[ix]+(p[1]-p[2]*x0))\n",
    "                Act.append(y1[ix])\n",
    "\n",
    "            MSE_results.append(MSE(Act,Pred))\n",
    "        if Line==1:\n",
    "            INDEX= np.where(x1>=x0)[0]\n",
    "            for ix in INDEX:\n",
    "                Pred.append( p[3]*x1[ix]+(p[1]-p[3]*x0))\n",
    "                Act.append(y1[ix])\n",
    "\n",
    "            MSE_results.append(MSE(Act,Pred))\n",
    "    ax3.scatter(x_bin[i], y_bin[i], color=color[i], s=10, marker=sym[i], facecolors='none')\n",
    "#     meanlineprops = dict(linestyle='--', linewidth=1., color=color[i])\n",
    "#     medianprops = dict(linestyle='-', linewidth=1, color=color[i])\n",
    "#     if i == 1:\n",
    "#         BOX2=ax3.boxplot(y_bin[i],meanprops=meanlineprops,medianprops=medianprops,showmeans=True, meanline=True,sym='',\n",
    "#                     positions=[1950, 2050, 2150, 2250, 2350, 2450, 2550, 2650, 2750, 2850, 2950, 3050],widths=75)\n",
    "#     else:\n",
    "#         BOX2=ax3.boxplot(y_bin[i],meanprops=meanlineprops,medianprops=medianprops,showmeans=True, meanline=True,sym='',\n",
    "#                     positions=[1950, 2050, 2150, 2250, 2350, 2450, 2550, 2650, 2750, 2850, 2950, 3050,3150,3250],widths=75)\n",
    "\n",
    "    ax3.text(0.25, ytxt[i], txt[i]+ ' L1: '+ str(np.round(p[2]*1000,2)) +' L2: ' + \n",
    "                    str(np.round(p[3]*1000,2)) + ' AABR: ' + str(np.round(p[2]/p[3],2)),\n",
    "                     transform=ax3.transAxes)  \n",
    "    ax3.legend(loc='best')\n",
    "#         print(MSE_results)\n",
    "fig3.savefig(fl_path + 'products/' + glacier[gl]+'_' + balance +'_all_years_combinedBIG.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILLECILLEWAET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## piecewise function ########################################         \n",
    "#     if year==2016:\n",
    "#         y_bin=[OBF,GPRq[1:],OPTq[1:],FARq[1:]]; x_bin=[z_range,z_range[1:],z_range[1:],z_range[1:]]\n",
    "#     else:\n",
    "#         y_bin=[OBF,GPRq,OPTq,FARq]; x_bin=[z_range,z_range,z_range,z_range]\n",
    "#     def piecewise_linear(x, x0, y0, k1, k2):\n",
    "#         x0=2440\n",
    "#         return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])   \n",
    "   \n",
    "#     for i in range(4):      \n",
    "# #         plt.figure(figsize=(10,8))\n",
    "#         x1=np.array(x_bin[i]); y1=np.array(y_bin[i])\n",
    "#         p , e = curve_fit(piecewise_linear, x1, y1 )\n",
    "#         if year == 2016:\n",
    "#             xd = np.linspace(2250, 2700, 31) #xd = np.arange(1900, 2950, 50)    ## \n",
    "#         else:\n",
    "#             xd = np.linspace(2050, 2700, 31) \n",
    "#         ax1[count].plot(xd, piecewise_linear(xd, *p), color=color[i])\n",
    "#         # mean square error of the lines\n",
    "#         x0=2440\n",
    "\n",
    "#         MSE_results=[]\n",
    "#         for Line in range(2):\n",
    "#             Pred=[];Act=[]\n",
    "#             if Line ==0:\n",
    "#                 INDEX= np.where(x1<x0)[0]\n",
    "#                 for ix in INDEX:\n",
    "#                     Pred.append( p[2]*x1[ix]+(p[1]-p[2]*x0))\n",
    "#                     Act.append(y1[ix])\n",
    "\n",
    "#                 MSE_results.append(MSE(Act,Pred))\n",
    "#             if Line==1:\n",
    "#                 INDEX= np.where(x1>=x0)[0]\n",
    "#                 for ix in INDEX:\n",
    "#                     Pred.append( p[3]*x1[ix]+(p[1]-p[3]*x0))\n",
    "#                     Act.append(y1[ix])\n",
    "\n",
    "#                 MSE_results.append(MSE(Act,Pred))\n",
    "#                 ax1[count].text(0.2, ytxt[i], txt[i]+ ' L1: '+ str(np.round(p[2]*1000,2)) +' L2: ' + \n",
    "#                         str(np.round(p[3]*1000,2)) + ' AABR: ' + str(round(p[2]/p[3],2)), transform=ax1[count].transAxes,\n",
    "#                                 fontsize=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
